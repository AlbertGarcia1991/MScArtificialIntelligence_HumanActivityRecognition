{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>City, University of London - MSc Artificial Intelligence\n",
    "# <center>INM701 - COURSEWORK\n",
    "## LSTM algorithm\n",
    "### Student:\n",
    ">Garc√≠a Plaza, Albert\n",
    "\n",
    "***\n",
    "\n",
    "The present notebook has been written in order to classify the data using a Long-Short Term Memory network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have worked on Google's clod computing server _GOOGLE COLAB_, so the next cell is only valid to work in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOAD FILES FROM GOOGLE DRIVE TO RUN CODE IN GOOGLE COLAB #####\n",
    "# Code to read csv file into Colaboratory:!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Google Drive shareable links\n",
    "pa_link = 'https://drive.google.com/open?id=12x7DjbbMAVlEdt2yPG5oqDVQXJQcKso0'\n",
    "pg_link = 'https://drive.google.com/open?id=1pwjF27YRF1Pnll8zh7QK_LI3CfS2id81'\n",
    "wa_link = 'https://drive.google.com/open?id=1c2Fz-ULrtOwcBcRQ3yTPoARApoxiCXRF'\n",
    "wg_link = 'https://drive.google.com/open?id=1XEPvoiQEGhOcr-dXqi33rbY5TWQ3MlMy'\n",
    "all_link = 'https://drive.google.com/open?id=1yTO_ZHBvJpBmuaYTSZcwrVbkdj_ybZs4'\n",
    "\n",
    "# Verify that you have everything after '='\n",
    "fluff, pa_id = pa_link.split('=') \n",
    "fluff, pg_id = pg_link.split('=')\n",
    "fluff, wa_id = wa_link.split('=')\n",
    "fluff, wg_id = wg_link.split('=')\n",
    "fluff, all_id = all_link.split('=')\n",
    "\n",
    "# Load datasets into Python session\n",
    "pa_downloaded = drive.CreateFile({'id':pa_id}) \n",
    "pa_downloaded.GetContentFile('Filename.pkl')  \n",
    "pa_prep = pd.read_pickle('Filename.pkl')\n",
    "pg_downloaded = drive.CreateFile({'id':pg_id}) \n",
    "pg_downloaded.GetContentFile('Filename.pkl')  \n",
    "pg_prep = pd.read_pickle('Filename.pkl')\n",
    "wa_downloaded = drive.CreateFile({'id':wa_id}) \n",
    "wa_downloaded.GetContentFile('Filename.pkl')  \n",
    "wa_prep = pd.read_pickle('Filename.pkl')\n",
    "wg_downloaded = drive.CreateFile({'id':wg_id}) \n",
    "wg_downloaded.GetContentFile('Filename.pkl')  \n",
    "wg_prep = pd.read_pickle('Filename.pkl')\n",
    "all_downloaded = drive.CreateFile({'id':all_id}) \n",
    "all_downloaded.GetContentFile('Filename.pkl')  \n",
    "all_prep = pd.read_pickle('Filename.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work on local directory, the next cell has to been used to load the data, instead of previously cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOAD FILES FROM LOCAL DIRECTORY #####\n",
    "pa_prep = pd.read_pickle('pa_prep.pkl')\n",
    "pg_prep = pd.read_pickle('pg_prep.pkl')\n",
    "wa_prep = pd.read_pickle('wa_prep.pkl')\n",
    "wg_prep = pd.read_pickle('wg_prep.pkl')\n",
    "all_prep = pd.read_pickle('all_prep.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last minute data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only original columns (from raw data) of each dataset and change their names\n",
    "pa_prep = pa_prep.drop(columns=['P_LO_x', 'P_LO_y', 'P_LO_z'])\n",
    "pg_prep = pg_prep.drop(columns=['P_AA_x', 'P_AA_y', 'P_AA_z', 'P_AO_x', 'P_AO_y', 'P_AO_z'])\n",
    "wa_prep = wa_prep.drop(columns=['W_LO_x', 'W_LO_y', 'W_LO_z'])\n",
    "wg_prep = wg_prep.drop(columns=['W_AA_x', 'W_AA_y', 'W_AA_z', 'W_AO_x', 'W_AO_y', 'W_AO_z'])\n",
    "pa_prep = pa_prep.rename(columns={'P_LA_x':'x', 'P_LA_y':'y', 'P_LA_z':'z'})\n",
    "pg_prep = pg_prep.rename(columns={'P_AV_x':'x', 'P_AV_y':'y', 'P_AV_z':'z'})\n",
    "wa_prep = wa_prep.rename(columns={'W_LA_x':'x', 'W_LA_y':'y', 'W_LA_z':'z'})\n",
    "wg_prep = wg_prep.rename(columns={'W_AV_x':'x', 'W_AV_y':'y', 'W_AV_z':'z'})\n",
    "\n",
    "# Set the window of 200 entries\n",
    "WINDOW = 200\n",
    "\n",
    "# Create a tuple with the correct shape to be readed by the LSTM\n",
    "X, y = [], []\n",
    "for i in range(51):\n",
    "    for j in range(18):\n",
    "        for k in range(13):\n",
    "            start_index = 500 + k * WINDOW\n",
    "            end_index = start_index + WINDOW\n",
    "            user_lstm = wa_prep[wa_prep.user == i]\n",
    "            to_append = user_lstm[user_lstm.activity == j][start_index:end_index]\n",
    "            xs = to_append['x'].values\n",
    "            ys = to_append['y'].values\n",
    "            zs = to_append['z'].values\n",
    "            X.append([xs, ys, zs])\n",
    "            y.append(j)           \n",
    "X_prep = np.zeros((11934, WINDOW, 3))\n",
    "for i in range(11934):\n",
    "    for j in range(200):\n",
    "        for k in range(3):\n",
    "            try: X_prep[i][j][k] = X[i][k][j]\n",
    "            except: pass\n",
    "y_prep = np.asarray(pd.get_dummies(y), dtype = np.float32)\n",
    "\n",
    "# Split the data into a 20% for testing (last 10 users with all activities)\n",
    "X_train = X_prep[:9594]\n",
    "X_test = X_prep[9594:]\n",
    "y_train = y_prep[:9594]\n",
    "y_test = y_prep[9594:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main functions declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LSTM with the specified number of hidden units\n",
    "def create_LSTM(inputs):\n",
    "    '''Given the array inputs as only argument, the function creates a LSTM with one hidden layer and the specified\n",
    "    number or hidden neurons (units), with random initialization of weights and biases (as dictionaries)'''\n",
    "    \n",
    "    # Weights and biases initialization\n",
    "    W = {'hidden': tf.Variable(tf.random_normal([N_FEATURES, N_HIDDEN_UNITS])),\n",
    "         'output': tf.Variable(tf.random_normal([N_HIDDEN_UNITS, N_CLASSES]))}\n",
    "    B = {'hidden': tf.Variable(tf.random_normal([N_HIDDEN_UNITS], mean=1.0)),\n",
    "         'output': tf.Variable(tf.random_normal([N_CLASSES]))}\n",
    "    \n",
    "    # Feed-forward pass\n",
    "    X = tf.transpose(inputs, [1, 0, 2])\n",
    "    X = tf.reshape(X, [-1, N_FEATURES])\n",
    "    hidden = tf.nn.relu(tf.matmul(X, W['hidden']) + B['hidden'])\n",
    "    hidden = tf.split(hidden, N_TIME_STEPS, 0)\n",
    "\n",
    "    # Stack 2 LSTM layers\n",
    "    lstm_layers = [tf.contrib.rnn.BasicLSTMCell(N_HIDDEN_UNITS, forget_bias=1.0) for _ in range(2)]\n",
    "    lstm_layers = tf.contrib.rnn.MultiRNNCell(lstm_layers)\n",
    "    \n",
    "    # Operate with all timesteps\n",
    "    outputs, _ = tf.contrib.rnn.static_rnn(lstm_layers, hidden, dtype=tf.float32)\n",
    "\n",
    "    # Output last timestep\n",
    "    lstm_last_output = outputs[-1]\n",
    "\n",
    "    return tf.matmul(lstm_last_output, W['output']) + B['output']\n",
    "\n",
    "# Print LSTM results and plots\n",
    "def print_results(history, y_test, predictions, f1scores, plot):\n",
    "    '''Passing values for train/test loss and accuracy, calculates the macro-averaged F1 Score.\n",
    "    If f1scores argument is True, print the F1 Scores for all activities; if plot is True, shows \n",
    "    loss and accuracy line plots and confusion matrix'''\n",
    "    \n",
    "    # Generate the confusion matrix tuple\n",
    "    max_test = np.argmax(y_test, axis=1)\n",
    "    max_predictions = np.argmax(predictions, axis=1)\n",
    "    confusion_matrix = metrics.confusion_matrix(max_test, max_predictions)\n",
    "    \n",
    "    # Computation of the F1 Scores (and macro-averaged F1 Score) through data in confusion matrix tuple\n",
    "    recall = []\n",
    "    precision = []\n",
    "    f1s = []\n",
    "    for i in range(18):\n",
    "        recall.append(confusion_matrix[i][i] / confusion_matrix[i].sum())\n",
    "        precision.append(confusion_matrix[i][i] / confusion_matrix.T[i].sum())\n",
    "        f1 = (2 * precision[-1] * recall[-1])/(precision[-1] + recall[-1])\n",
    "        f1s.append(f1)\n",
    "    f1s = np.array(f1s)\n",
    "    f1s[np.isnan(f1s)] = 0\n",
    "    \n",
    "    # Print macro-averaged F1 Score\n",
    "    print(\"Macro F1 Score: %.4f\" %f1s.mean())\n",
    "    \n",
    "    # Print F1 Score for all activities when f1scores=True\n",
    "    if f1scores:\n",
    "        print(f1s)\n",
    "    \n",
    "    # Plot train/test loss and accuracy vs. epochs when plot=True\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(np.array(history['train_loss']), \"r-\", label=\"Train loss\")\n",
    "        plt.plot(np.array(history['test_loss']), \"g-\", label=\"Test loss\")\n",
    "        plt.title(\"Training and Test Loss over Iterarions\")\n",
    "        plt.ylabel('Loss value)')\n",
    "        plt.xlabel('Training Epoch')\n",
    "        plt.ylim(0)\n",
    "        plt.xlim(0,N_EPOCHS)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(np.array(history['train_acc']), \"r-\", label=\"Train accuracy\")\n",
    "        plt.plot(np.array(history['test_acc']), \"g-\", label=\"Test accuracy\")\n",
    "        plt.title(\"Training and Test Accuracy over Iterarions\")\n",
    "        plt.legend(loc='upper right', shadow=True)\n",
    "        plt.ylabel('Accuracy values')\n",
    "        plt.xlabel('Training Epoch')\n",
    "        plt.ylim(0,1)\n",
    "        plt.xlim(0,N_EPOCHS)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(16, 14))\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(\"Confusion matrix\")\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "\n",
    "# Main function which creates the LSTM, runs the training process and shows results\n",
    "def LSTM_train(N_CLASSES, N_HIDDEN_UNITS, N_TIME_STEPS, N_FEATURES, N_EPOCHS, BATCH_SIZE, LEARNING_RATE, L2_LOSS, f1scores=False, plot=False): \n",
    "    # Reset current TensorFlow Graph (clear sets of TensorFlow Operations and Tensors)\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Initialize TensorFlow Placeholders (variables that will be assigned data later) for input and output values as float\n",
    "    X = tf.placeholder(tf.float32, [None, N_TIME_STEPS, N_FEATURES])\n",
    "    Y = tf.placeholder(tf.float32, [None, N_CLASSES])\n",
    "\n",
    "    # Computation of feed-forward prograpation\n",
    "    pred_Y = create_LSTM(X)\n",
    "    pred_softmax = tf.nn.softmax(pred_Y, name=\"y_\")\n",
    "\n",
    "    # Computation of L2 loss and total loss\n",
    "    l2 = L2_LOSS * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred_Y, labels = Y)) + l2\n",
    "\n",
    "    # Computation of backpropagation\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "    correct_pred = tf.equal(tf.argmax(pred_softmax, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Initialize the dictionaries where loss and accuracy history will be saved\n",
    "    history = dict(train_loss=[], train_acc=[], test_loss=[], test_acc=[])\n",
    "\n",
    "    # Start TensorFlow session\n",
    "    sess=tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Iterate over specified EPOCHS \n",
    "    train_count = len(X_train)\n",
    "    for i in range(1, N_EPOCHS + 1):\n",
    "        for start, end in zip(range(0, train_count, BATCH_SIZE), range(BATCH_SIZE, train_count + 1,BATCH_SIZE)):\n",
    "            sess.run(optimizer, feed_dict={X: X_train[start:end], Y: y_train[start:end]})\n",
    "\n",
    "        _, acc_train, loss_train = sess.run([pred_softmax, accuracy, loss], feed_dict={\n",
    "                                                X: X_train, Y: y_train})\n",
    "\n",
    "        _, acc_test, loss_test = sess.run([pred_softmax, accuracy, loss], feed_dict={\n",
    "                                                X: X_test, Y: y_test})\n",
    "\n",
    "        history['train_loss'].append(loss_train)\n",
    "        history['train_acc'].append(acc_train)\n",
    "        history['test_loss'].append(loss_test)\n",
    "        history['test_acc'].append(acc_test)\n",
    "\n",
    "        if i==1 or i%5 == 0:\n",
    "            print(f'Epoch: {i} --- test accuracy: {acc_test} --- loss: {loss_test}')\n",
    "    \n",
    "    # Last run to obtain final results\n",
    "    predictions, acc_final, loss_final = sess.run([pred_softmax, accuracy, loss], feed_dict={X: X_test, Y: y_test})\n",
    "    \n",
    "    # Show results\n",
    "    print(f'Final results: accuracy: {acc_final} loss: {loss_final}')\n",
    "    print_results(history, y_test, predictions, f1scores, plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters setting and run LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the network\n",
    "N_CLASSES = len(y_train[0])\n",
    "N_HIDDEN_UNITS = 60\n",
    "N_TIME_STEPS = X_train.shape[1]\n",
    "N_FEATURES = X_train.shape[2]\n",
    "\n",
    "# Performance hyper-parameters\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 500\n",
    "LEARNING_RATE = 0.001\n",
    "L2_LOSS = 0.02\n",
    "\n",
    "# Result flags\n",
    "f1scores = False\n",
    "plot = False\n",
    "\n",
    "# Run training process\n",
    "LSTM_train(N_CLASSES, N_HIDDEN_UNITS, N_TIME_STEPS, N_FEATURES, N_EPOCHS, BATCH_SIZE, LEARNING_RATE, L2_LOSS, f1scores, plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}